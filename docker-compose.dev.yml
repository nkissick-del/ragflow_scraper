services:
  scraper:
    build:
      context: .
      dockerfile: Dockerfile
      target: dev-runtime
      labels:
        org.opencontainers.image.title: "pdf-scraper-dev"
        org.opencontainers.image.description: "Dev runtime for PDF scraper"
        org.opencontainers.image.revision: "local"
    container_name: pdf-scraper-dev
    restart: unless-stopped
    ports:
      - "5001:5000"
    volumes:
      # Mount source code for live reloading
      - ./app:/app/app
      - ./scripts:/app/scripts
      - ./docs:/app/docs
      - ./tests:/app/tests
      - ./data:/app/data
      - ./config:/app/config
    env_file:
      - .env
    environment:
      - FLASK_ENV=development
      - FLASK_DEBUG=1
      - SECRET_KEY=dev-secret-key
      - HOST=0.0.0.0
      - PORT=5000
      # Selenium Configuration (override for docker networking)
      - SELENIUM_REMOTE_URL=http://chrome:4444/wd/hub
      - SELENIUM_HEADLESS=true
      # Directories (container paths)
      - DOWNLOAD_DIR=/app/data/scraped
      - METADATA_DIR=/app/data/metadata
      - STATE_DIR=/app/data/state
      - LOG_DIR=/app/data/logs
      - LOG_LEVEL=DEBUG
    depends_on:
      chrome:
        condition: service_healthy
    networks:
      - scraper-net
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: "0.75"

  chrome:
    image: seleniarm/standalone-chromium:120.0
    platform: linux/arm64
    container_name: pdf-scraper-chrome-dev
    restart: unless-stopped
    ports:
      - "4444:4444"
      - "7900:7900"
    shm_size: 2gb
    environment:
      - SE_NODE_MAX_SESSIONS=3
      - SE_NODE_SESSION_TIMEOUT=300
      - SE_VNC_NO_PASSWORD=1
    networks:
      - scraper-net
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4444/wd/hub/status"]
      interval: 15s
      timeout: 10s
      retries: 3
      start_period: 10s

networks:
  scraper-net:
    driver: bridge
