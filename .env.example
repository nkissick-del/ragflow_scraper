# ==============================================================================
# RAGFlow Scraper Environment Configuration
# ==============================================================================
# Instructions:
# 1. Copy this file to .env: `cp .env.example .env`
# 2. Replace all "your_*_here" and "YOUR_*_HERE" placeholders with real values.
# 3. NEVER commit your real .env file with actual credentials to version control.
# ==============================================================================

# RAGFlow Configuration
RAGFLOW_API_URL=http://localhost:9380
RAGFLOW_API_KEY=your_api_key_here
RAGFLOW_DATASET_ID=your_dataset_id

# Paperless-ngx Configuration
# Required when ARCHIVE_BACKEND=paperless
PAPERLESS_API_URL=http://localhost:8000
PAPERLESS_API_TOKEN=your_paperless_token_here

# AnythingLLM Configuration (alternative RAG backend)
# Required when RAG_BACKEND=anythingllm
ANYTHINGLLM_API_URL=http://localhost:3001
ANYTHINGLLM_API_KEY=your_anythingllm_key_here
ANYTHINGLLM_WORKSPACE_ID=your_workspace_id

# Backend Selection (modular architecture)
PARSER_BACKEND=docling  # Options: docling, docling_serve, mineru, tika
ARCHIVE_BACKEND=paperless  # Options: paperless, s3, local (default requires Paperless-ngx configuration above)
RAG_BACKEND=ragflow  # Options: ragflow, anythingllm, pgvector
METADATA_MERGE_STRATEGY=smart  # Options: smart, parser_wins, scraper_wins

# S3 Archive Backend Configuration
# Required when ARCHIVE_BACKEND=s3
AWS_ACCESS_KEY_ID=your_aws_key_here
AWS_SECRET_ACCESS_KEY=your_aws_secret_here
AWS_REGION=us-east-1
S3_BUCKET=your_bucket_name_here
# S3_ENDPOINT_URL=http://localhost:9000  # Optional (e.g., for MinIO)

# Local Archive Backend Configuration
# Required when ARCHIVE_BACKEND=local
ARCHIVE_LOCAL_PATH=/app/data/archive

# MinerU Parser Configuration
# Required when PARSER_BACKEND=mineru
# MINERU_API_KEY=your_mineru_key_here
# MINERU_URL=http://localhost:8000

# Tika Parser Configuration
# Required when PARSER_BACKEND=tika
# TIKA_SERVER_URL=http://localhost:9998
# TIKA_TIMEOUT=120
# TIKA_ENRICHMENT_ENABLED=false

# Gotenberg (document -> PDF conversion for archiving)
# Used to convert HTML/Markdown/Office documents to PDF before uploading to Paperless.
# In Docker: http://gotenberg:3000 (set automatically by docker-compose)
# Local/external: http://localhost:3156
GOTENBERG_URL=http://gotenberg:3000
# GOTENBERG_TIMEOUT=60

# Docling-serve Parser Configuration (HTTP REST API)
# Required when PARSER_BACKEND=docling_serve
# DOCLING_SERVE_URL=http://localhost:4949
# DOCLING_SERVE_TIMEOUT=300

# Note: Many backends (like docling) require no extra env vars beyond defaults.
# See external documentation for each backend provider for more details.

# pgvector RAG Backend Configuration
# Required when RAG_BACKEND=pgvector
# DATABASE_URL=postgresql://user:password@localhost:5432/scraper_vectors

# Embedding Service Configuration (required for pgvector RAG backend)
# EMBEDDING_BACKEND=ollama  # Options: ollama, openai, api
# EMBEDDING_MODEL=nomic-embed-text
# EMBEDDING_URL=http://localhost:11434
# EMBEDDING_API_KEY=  # Only needed for openai/api backends
# EMBEDDING_DIMENSIONS=768
# EMBEDDING_TIMEOUT=60

# Chunking Configuration (for pgvector RAG backend)
# CHUNKING_STRATEGY=fixed  # Options: fixed, hybrid
# CHUNK_MAX_TOKENS=512
# CHUNK_OVERLAP_TOKENS=64

# LLM Service Configuration (for document enrichment & contextual embeddings)
# Uses same Ollama instance as embeddings by default (LLM_URL falls back to EMBEDDING_URL)
# LLM_BACKEND=ollama  # Options: ollama, openai, api
# LLM_MODEL=llama3.1:8b
# LLM_URL=  # Leave empty to use EMBEDDING_URL
# LLM_API_KEY=  # Only needed for openai/api backends
# LLM_TIMEOUT=120
# LLM_ENRICHMENT_ENABLED=false  # Tier 1: document-level metadata extraction
# LLM_ENRICHMENT_MAX_TOKENS=8000
# CONTEXTUAL_ENRICHMENT_ENABLED=false  # Tier 2: chunk-level contextual descriptions
# CONTEXTUAL_ENRICHMENT_WINDOW=3

# FlareSolverr Configuration
FLARESOLVERR_URL=http://localhost:8191

# Flask Configuration
FLASK_ENV=production  # Override to 'development' for local dev
# Set to 1 for development only â€” NEVER enable in production
FLASK_DEBUG=0
# Generate with: python3 -c "import secrets; print(secrets.token_hex(32))"
SECRET_KEY=REPLACE_WITH_GENERATED_SECRET
HOST=0.0.0.0
PORT=5000

# Basic Auth (required in production; see SECRETS_ROTATION.md)
BASIC_AUTH_ENABLED=true
BASIC_AUTH_USERNAME=admin
BASIC_AUTH_PASSWORD=change_me_to_a_strong_password

# Selenium Configuration (used for web scraping only, not PDF generation)
# Note: Default assumes Docker/containerized environment. For local setup, use http://localhost:4444/wd/hub
SELENIUM_REMOTE_URL=http://chrome:4444/wd/hub
SELENIUM_HEADLESS=true

# Scraper Configuration
# Note: Paths below assume Docker/containerized environment. For local setup, use host filesystem paths
# (e.g., ./data/scraped, ./data/metadata, etc.) and ensure directories exist with proper permissions.
DOWNLOAD_DIR=/app/data/scraped
METADATA_DIR=/app/data/metadata
STATE_DIR=/app/data/state
LOG_DIR=/app/data/logs
MAX_CONCURRENT_DOWNLOADS=3
REQUEST_TIMEOUT=60
RETRY_ATTEMPTS=3

# Logging
LOG_LEVEL=INFO

# VLM Configuration (OpenRouter)
# VLM processing is optional and used for graph/image extraction from PDFs.
# Note: Using OpenRouter API has billing implications and rate limits.
OPENROUTER_API_KEY=sk-or-v1-your_key_here
ENABLE_VLM_PROCESSING=false
VLM_MODEL=qwen/qwen-2-vl-7b-instruct
# If VLM_MAX_GRAPHS_PER_DOC is exceeded, remaining graphs are skipped/truncated.
VLM_MAX_GRAPHS_PER_DOC=50

# Docling Settings
DOCLING_TIMEOUT=300  # timeout in seconds
PRESERVE_ORIGINAL_PDFS=true
