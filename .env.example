# ==============================================================================
# RAGFlow Scraper Environment Configuration
# ==============================================================================
# Instructions:
# 1. Copy this file to .env: `cp .env.example .env`
# 2. Replace all "your_*_here" and "YOUR_*_HERE" placeholders with real values.
# 3. NEVER commit your real .env file with actual credentials to version control.
# ==============================================================================

# RAGFlow Configuration
RAGFLOW_API_URL=http://localhost:9380
RAGFLOW_API_KEY=your_api_key_here
RAGFLOW_DATASET_ID=your_dataset_id

# Paperless-ngx Configuration
# Required when ARCHIVE_BACKEND=paperless
PAPERLESS_API_URL=http://localhost:8000
PAPERLESS_API_TOKEN=your_paperless_token_here

# AnythingLLM Configuration (alternative RAG backend)
# Required when RAG_BACKEND=anythingllm
ANYTHINGLLM_API_URL=http://localhost:3001
ANYTHINGLLM_API_KEY=your_anythingllm_key_here
ANYTHINGLLM_WORKSPACE_ID=your_workspace_id

# Backend Selection (modular architecture)
PARSER_BACKEND=docling  # Options: docling, docling_serve, mineru, tika
ARCHIVE_BACKEND=paperless  # Options: paperless, s3, local (default requires Paperless-ngx configuration above)
RAG_BACKEND=ragflow  # Options: ragflow, anythingllm, pgvector
METADATA_MERGE_STRATEGY=smart  # Options: smart, parser_wins, scraper_wins

# S3 Archive Backend Configuration
# Required when ARCHIVE_BACKEND=s3
AWS_ACCESS_KEY_ID=your_aws_key_here
AWS_SECRET_ACCESS_KEY=your_aws_secret_here
AWS_REGION=us-east-1
S3_BUCKET=your_bucket_name_here
# S3_ENDPOINT_URL=http://localhost:9000  # Optional (e.g., for MinIO)

# Local Archive Backend Configuration
# Required when ARCHIVE_BACKEND=local
ARCHIVE_LOCAL_PATH=/app/data/archive

# MinerU Parser Configuration
# Required when PARSER_BACKEND=mineru
# MINERU_API_KEY=your_mineru_key_here
# MINERU_URL=http://localhost:8000

# Tika Parser Configuration
# Required when PARSER_BACKEND=tika
# TIKA_SERVER_URL=http://localhost:9998
# TIKA_TIMEOUT=120
# TIKA_ENRICHMENT_ENABLED=false

# Gotenberg (document -> PDF conversion for archiving)
# Used to convert HTML/Markdown/Office documents to PDF before uploading to Paperless.
# In Docker: http://gotenberg:3000 (set automatically by docker-compose)
# Local/external: http://localhost:3156
GOTENBERG_URL=http://gotenberg:3000
# GOTENBERG_TIMEOUT=60

# Docling-serve Parser Configuration (HTTP REST API)
# Required when PARSER_BACKEND=docling_serve
# DOCLING_SERVE_URL=http://localhost:4949
# DOCLING_SERVE_TIMEOUT=300

# Note: Many backends (like docling) require no extra env vars beyond defaults.
# See external documentation for each backend provider for more details.

# pgvector RAG Backend Configuration
# Required when RAG_BACKEND=pgvector
# DATABASE_URL=postgresql://user:password@localhost:5432/scraper_vectors

# Embedding Service Configuration (required for pgvector RAG backend)
# EMBEDDING_BACKEND=ollama  # Options: ollama, openai, api
# EMBEDDING_MODEL=nomic-embed-text
# EMBEDDING_URL=http://localhost:11434
# EMBEDDING_API_KEY=  # Only needed for openai/api backends
# EMBEDDING_DIMENSIONS=768
# EMBEDDING_TIMEOUT=60

# Chunking Configuration (for pgvector RAG backend)
# CHUNKING_STRATEGY=fixed  # Options: fixed, hybrid
# CHUNK_MAX_TOKENS=512
# CHUNK_OVERLAP_TOKENS=64

# LLM Service Configuration (for document enrichment & contextual embeddings)
# Uses same Ollama instance as embeddings by default (LLM_URL falls back to EMBEDDING_URL)
# LLM_BACKEND=ollama  # Options: ollama, openai, api
# LLM_MODEL=llama3.1:8b
# LLM_URL=  # Leave empty to use EMBEDDING_URL
# LLM_API_KEY=  # Only needed for openai/api backends
# LLM_TIMEOUT=120
# LLM_ENRICHMENT_ENABLED=false  # Tier 1: document-level metadata extraction
# LLM_ENRICHMENT_MAX_TOKENS=8000
# CONTEXTUAL_ENRICHMENT_ENABLED=false  # Tier 2: chunk-level contextual descriptions
# CONTEXTUAL_ENRICHMENT_WINDOW=3

# FlareSolverr Configuration
FLARESOLVERR_URL=http://localhost:8191

# Flask Configuration
FLASK_ENV=production  # Override to 'development' for local dev
# Set to 1 for development only â€” NEVER enable in production
FLASK_DEBUG=0
# Generate with: python3 -c "import secrets; print(secrets.token_hex(32))"
SECRET_KEY=REPLACE_WITH_GENERATED_SECRET
HOST=0.0.0.0
PORT=5000

# Basic Auth (required in production; see SECRETS_ROTATION.md)
BASIC_AUTH_ENABLED=true
BASIC_AUTH_USERNAME=admin
BASIC_AUTH_PASSWORD=change_me_to_a_strong_password

# Scraper Configuration
# Note: Paths below assume Docker/containerized environment. For local setup, use host filesystem paths
# (e.g., ./data/scraped, ./data/metadata, etc.) and ensure directories exist with proper permissions.
DOWNLOAD_DIR=/app/data/scraped
METADATA_DIR=/app/data/metadata
STATE_DIR=/app/data/state
LOG_DIR=/app/data/logs
MAX_CONCURRENT_DOWNLOADS=3
REQUEST_TIMEOUT=60
RETRY_ATTEMPTS=3

# Logging
LOG_LEVEL=INFO
# LOG_JSON_FORMAT=true  # JSON lines format for structured logging
# LOG_FILE_MAX_BYTES=10485760  # 10 MB max per log file
# LOG_FILE_BACKUP_COUNT=5  # Number of rotated log files to keep
# LOG_TO_FILE=true  # Set false to disable file logging

# RAGFlow Session Auth (for model listing and admin APIs)
# RAGFLOW_USERNAME=
# RAGFLOW_PASSWORD=

# RAGFlow Metadata Settings
# RAGFLOW_PUSH_METADATA=true
# RAGFLOW_METADATA_TIMEOUT=10.0
# RAGFLOW_METADATA_POLL_INTERVAL=0.5
# RAGFLOW_METADATA_RETRIES=3
# RAGFLOW_CHECK_DUPLICATES=true

# FlareSolverr Timeouts
# FLARESOLVERR_TIMEOUT=60
# FLARESOLVERR_MAX_TIMEOUT=120

# Guardian Open Platform API (optional)
# GUARDIAN_API_KEY=

# File naming template (Jinja2 syntax)
# FILENAME_TEMPLATE={{ date_prefix }}_{{ org }}_{{ title | slugify }}{{ extension }}

# File size limits
# MAX_UPLOAD_FILE_SIZE=524288000  # 500 MB

# Reverse proxy configuration
# TRUST_PROXY_COUNT=0  # Set to 1 when behind a single reverse proxy

# pgvector advanced settings
# PGVECTOR_DROP_ON_MISMATCH=false  # Drop and recreate table on dimension mismatch (DANGER)
# VECTOR_BACKEND=pgvector
