services:
  scraper:
    build:
      context: .
      dockerfile: Dockerfile
      labels:
        org.opencontainers.image.title: "pdf-scraper"
        org.opencontainers.image.description: "Local PDF scraper with RAGFlow upload"
        org.opencontainers.image.revision: "local"
    container_name: pdf-scraper
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    ports:
      - "${PORT:-5000}:5000"
    volumes:
      - ./data:/app/data
      - ./config:/app/config
      - ./logs:/app/logs
    environment:
      # Flask Configuration
      - FLASK_ENV=${FLASK_ENV:-production}
      - FLASK_DEBUG=${FLASK_DEBUG:-0}
      - SECRET_KEY=${SECRET_KEY:-change-this-in-production}
      - HOST=0.0.0.0
      - PORT=5000
      # RAGFlow Configuration
      - RAGFLOW_API_URL=${RAGFLOW_API_URL:-http://localhost:9380}
      - RAGFLOW_API_KEY=${RAGFLOW_API_KEY:-}
      - RAGFLOW_DATASET_ID=${RAGFLOW_DATASET_ID:-}
      # Selenium Configuration
      - SELENIUM_REMOTE_URL=http://chrome:4444/wd/hub
      - SELENIUM_HEADLESS=true
      # Directories (inside container)
      - DOWNLOAD_DIR=/app/data/scraped
      - METADATA_DIR=/app/data/metadata
      - STATE_DIR=/app/data/state
      - LOG_DIR=/app/data/logs
      # Scraper Configuration
      - MAX_CONCURRENT_DOWNLOADS=${MAX_CONCURRENT_DOWNLOADS:-3}
      - REQUEST_TIMEOUT=${REQUEST_TIMEOUT:-60}
      - RETRY_ATTEMPTS=${RETRY_ATTEMPTS:-3}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # Guardian API
      - GUARDIAN_API_KEY=${GUARDIAN_API_KEY:-}
      # Gotenberg (document → PDF conversion for archiving)
      - GOTENBERG_URL=http://gotenberg:3000
    depends_on:
      chrome:
        condition: service_healthy
      gotenberg:
        condition: service_healthy
    networks:
      - scraper-net
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: "0.75"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Browser for web scraping (not used for PDF generation — see gotenberg)
  chrome:
    image: seleniarm/standalone-chromium:120.0
    platform: linux/arm64
    container_name: pdf-scraper-chrome
    restart: unless-stopped
    ports:
      - "${SELENIUM_PORT:-4444}:4444"
      - "${VNC_PORT:-7900}:7900"  # VNC for debugging (password: secret)
    shm_size: 2gb
    environment:
      - SE_NODE_MAX_SESSIONS=3
      - SE_NODE_SESSION_TIMEOUT=300
      - SE_VNC_NO_PASSWORD=1
    networks:
      - scraper-net
    security_opt:
      - no-new-privileges:true
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4444/wd/hub/status"]
      interval: 15s
      timeout: 10s
      retries: 3
      start_period: 10s

  gotenberg:
    image: gotenberg/gotenberg:8
    container_name: pdf-scraper-gotenberg
    restart: unless-stopped
    security_opt:
      - no-new-privileges:true
    ports:
      - "${GOTENBERG_PORT:-3156}:3000"
    networks:
      - scraper-net
    deploy:
      resources:
        limits:
          memory: 512m
          cpus: "0.50"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

networks:
  scraper-net:
    driver: bridge

volumes:
  scraper-data:
  scraper-config:
  scraper-logs:
